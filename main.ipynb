{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieval-Augmented Generation (RAG) QA Bot\n",
        "This notebook demonstrates the implementation of a Retrieval-Augmented Generation (RAG) model for a Question-Answering (QA) bot using OpenAI API and Pinecone. The bot retrieves relevant information from a vector database and generates natural language answers.\n",
        "\n",
        "## Key Features:\n",
        "- Stores business-related documents in a Pinecone vector database.\n",
        "- Retrieves the most relevant documents based on user queries.\n",
        "- Generates human-like answers using OpenAI's GPT model.\n"
      ],
      "metadata": {
        "id": "TmN4PDyD-K2P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBL844FhaDFJ",
        "outputId": "9d1dcd8c-eb44-4644-a34a-ec91b512fecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.12.14)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "Requirement already satisfied: openai==0.27.0 in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.27.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.0) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.0) (3.11.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.0) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.0) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.27.0) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone-client\n",
        "!pip install openai==0.27.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Dependencies\n",
        "Before running this notebook, ensure you have:\n",
        "- A valid Pinecone API key and environment.\n",
        "- An OpenAI API key.\n",
        "- Installed the required libraries (`pinecone`, `openai`).\n",
        "\n",
        "This section installs and initializes the required libraries.\n"
      ],
      "metadata": {
        "id": "Om9D_R3l-a_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from pinecone import Pinecone, ServerlessSpec"
      ],
      "metadata": {
        "id": "xSExXrMecwMM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = \"your_pinecone_API_key\"\n",
        "PINECONE_ENVIRONMENT = \"pinecone_environment\"\n",
        "OPENAI_API_KEY = \"your_OpenAI_API_key\"\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "x7tMdyo8bnXD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinecone Setup\n",
        "Pinecone is a vector database used to store embeddings of documents. We initialize Pinecone with our API key and create an index to manage the data.\n",
        "\n",
        "Key steps:\n",
        "- Connect to Pinecone using the API key.\n",
        "- Create an index for storing document embeddings.\n",
        "- Ensure the index supports the required dimensionality and metric type.\n"
      ],
      "metadata": {
        "id": "qqzq3nzU_rIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone instance\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)"
      ],
      "metadata": {
        "id": "tTaW6jL0feMZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INDEX_NAME = \"qa-bot-index\"\n",
        "DIMENSION = 1536\n",
        "\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.Index(name=INDEX_NAME)\n"
      ],
      "metadata": {
        "id": "c_lMuCXMfhT1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create text embeddings\n",
        "def embed_text(text):\n",
        "    response = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
        "    return response['data'][0]['embedding']\n"
      ],
      "metadata": {
        "id": "daR1e-vefi0L"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Preparation\n",
        "This section demonstrates how to add documents to the Pinecone index. Each document contains an `id` and `content`. The content is embedded using OpenAI's text-embedding model and stored in the vector database.\n",
        "\n",
        "Steps:\n",
        "- Create a list of documents relevant to the business domain.\n",
        "- Generate embeddings using OpenAI API.\n",
        "- Upsert (insert or update) the embeddings into the Pinecone index.\n"
      ],
      "metadata": {
        "id": "kBIZRm0z_kW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example documents to upsert\n",
        "documents = [\n",
        "    {\"id\": \"doc1\", \"content\": \"We specialize in AI solutions.\"},\n",
        "    {\"id\": \"doc2\", \"content\": \"Our office is in Mumbai.\"},\n",
        "    {\"id\": \"doc3\", \"content\": \"We offer cloud computing services.\"},\n",
        "    {\"id\": \"doc4\", \"content\": \"Customer support is available 24/7.\"},\n",
        "    {\"id\": \"doc5\", \"content\": \"Our products include chatbots, recommendation engines, and sentiment analysis tools.\"},\n",
        "    {\"id\": \"doc6\", \"content\": \"We have been recognized as a leader in AI innovation.\"},\n",
        "    {\"id\": \"doc7\", \"content\": \"Our mission is to make AI accessible to small businesses.\"},\n",
        "    {\"id\": \"doc8\", \"content\": \"We support integrations with leading cloud platforms like AWS, Azure, and Google Cloud.\"},\n",
        "    {\"id\": \"doc9\", \"content\": \"Our CEO, Manish Kumar, has 15 years of experience in AI and machine learning.\"},\n",
        "]\n",
        "\n",
        "# Upsert documents to Pinecone index\n",
        "for doc in documents:\n",
        "    embedding = embed_text(doc[\"content\"])\n",
        "    index.upsert([{\n",
        "        \"id\": doc[\"id\"],\n",
        "        \"values\": embedding,\n",
        "        \"metadata\": {\"content\": doc[\"content\"]}\n",
        "    }])\n"
      ],
      "metadata": {
        "id": "lNdBnEVofmEg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer Generation\n",
        "OpenAI's GPT model is used to generate answers by combining the retrieved document content and the user query. This enhances the bot's ability to provide contextually relevant and accurate answers.\n",
        "\n",
        "Key points:\n",
        "- Combine retrieved document data with user queries.\n",
        "- Generate responses using OpenAI GPT models.\n"
      ],
      "metadata": {
        "id": "dCi6g36PAN1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve top matching documents\n",
        "def retrieve_documents(query, top_k=3):\n",
        "    query_embedding = embed_text(query)\n",
        "    results = index.query(\n",
        "        vector=query_embedding,\n",
        "        top_k=top_k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    return [result['metadata']['content'] for result in results['matches']]"
      ],
      "metadata": {
        "id": "QRfjvfrffmvw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a meaningful response using GPT\n",
        "def generate_response(query, documents):\n",
        "\n",
        "    # Combine the document content into a single string\n",
        "    context = \" \".join(documents)\n",
        "\n",
        "    # Creating the prompt for GPT\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful assistant for a business QA bot. Use the context provided below to answer the user's question.\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    User Query: {query}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # Generating the response using GPT\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    # Returning the generated answer\n",
        "    return response['choices'][0]['message']['content'].strip()\n"
      ],
      "metadata": {
        "id": "YpAAQQyZBuqw"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the QA Bot\n",
        "Use example queries to test the bot's performance. Ensure the retrieved documents and generated answers align with the business context.\n",
        "\n",
        "Example query:\n",
        "\"What renewable energy solutions do you provide?\"\n"
      ],
      "metadata": {
        "id": "UuXk1ScxAVf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = \"what kind of products do you offer? And what about the customer service?\"\n",
        "\n",
        "# Retrieve top-matching documents\n",
        "matches = retrieve_documents(query)\n",
        "print(\"Top Matches:\", matches)\n",
        "\n",
        "# Generate a meaningful response\n",
        "response = generate_response(query, matches)\n",
        "print(\"Generated Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fDZg7jaAU1x",
        "outputId": "733da66a-55e1-4731-b38c-22c6953171d2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Matches: ['Customer support is available 24/7.', 'Our products include chatbots, recommendation engines, and sentiment analysis tools.', 'We offer cloud computing services.']\n",
            "Generated Response: We offer a variety of products including chatbots, recommendation engines, and sentiment analysis tools. Additionally, we provide cloud computing services. In relation to customer service, it is available 24/7 to assist with any needs or inquiries you may have.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "This notebook demonstrates the integration of Pinecone and OpenAI to build a RAG-based QA bot. By combining vector search with language generation, the bot provides precise and human-like answers to user queries.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9szSa5xYAYDR"
      }
    }
  ]
}